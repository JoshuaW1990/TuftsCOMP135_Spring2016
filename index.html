<h1 id="introduction-to-machine-learning-and-data-mining">Introduction to Machine Learning and Data Mining</h1>
<p>Comp 135: Introduction to Machine Learning and Data Mining<br />Department of Computer Science<br />Tufts University<br />Spring 2016</p>
<p>Course Web Page (redirects to current page): http://github.com/kephale/TuftsCOMP135_Spring2016</p>
<h1 id="announcements">Announcement(s):</h1>
<p>None</p>
<h1 id="what-is-this-course-about">What is this course about?</h1>
<p>Machine learning is the study of algorithmic methods for learning and prediction based upon data. Approaches range from extracting patterns from large collections of data, such as social media and scientific datasets, to online learning in real-time, for applications like active robots. ML is becoming increasingly widespread because of the increase and accessibility of computational power and datasets, as well as recent advances in ML algorithms. It is now commonplace for ML to produce results that have not been achieved by humans.</p>
<p>As this is an introductory course, we will focus on breadth over the field of ML, but this will still require significant cognitive effort on your part. The ideal candidate for this course is an upper-level undergraduate or beginning graduate student comfortable with some mathematical techniques and a solid grounding in programming. Maths that will prove useful are: statistics, probability, calculus, and linear algebra. We will review some of the essential topics, and the only explicit requirements are previous coursework of (COMP 15) and (COMP/MATH 22 or 61) or (consent of instructor). Comp 160 is highly recommended.</p>
<h2 id="class-times">Class Times:</h2>
<p>Tu, Th 10:30AM - 11:45AM<br />Tisch Library, 304-Auditorium</p>
<h2 id="instructor">Instructor:</h2>
<p>Kyle Harrington<br />Office Hours: By appointment</p>
<h2 id="teaching-assistants">Teaching Assistants:</h2>
<p>Sepideh Sadeghi</p>
<h1 id="grading">Grading</h1>
<ul>
<li>Written homework assignments (20%)<br /></li>
<li>Quizzes (20%)<br /></li>
<li>In-class midterm exam (20%): March 17</li>
<li>Final project (40%)</li>
</ul>
<h3 id="rules-for-late-submissions">Rules for late submissions:</h3>
<p>All work must be turned in on the date specified. Unless there is a last minute emergency, please notify <script type="text/javascript">
<!--
h='&#x65;&#x65;&#x63;&#x73;&#46;&#116;&#x75;&#102;&#116;&#x73;&#46;&#x65;&#100;&#x75;';a='&#64;';n='&#x6b;&#x79;&#108;&#x65;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+'Kyle Harrington'+'<\/'+'a'+'>');
// -->
</script><noscript>&#x4b;&#x79;&#108;&#x65;&#32;&#72;&#x61;&#114;&#114;&#x69;&#110;&#x67;&#116;&#x6f;&#110;&#32;&#40;&#x6b;&#x79;&#108;&#x65;&#32;&#x61;&#116;&#32;&#x65;&#x65;&#x63;&#x73;&#32;&#100;&#x6f;&#116;&#32;&#116;&#x75;&#102;&#116;&#x73;&#32;&#100;&#x6f;&#116;&#32;&#x65;&#100;&#x75;&#x29;</noscript> of special circumstances at least two days in advance.</p>
<p>If you aren't done by the due date, then turn in what you have finished for partial credit.</p>
<h1 id="collaboration">Collaboration</h1>
<p>On homework assignments and projects: Discussion about problems and concepts is great. Each assignment must be completed by you and only you, and is expected to be unique. Code should be written by you; writeups should be written by you. If you have collaborated (helping or being helped), just say so. There is no harm in saying so.</p>
<p>On quizzes and exams: no collaboration is allowed.</p>
<p>Failure to follow these guidelines may result in disciplinary action for all parties involved. For this and other issues concerning academic integrity please consult the booklet available from the office of the Dean of Student Affairs.</p>
<h1 id="tentative-list-of-topics">Tentative List of Topics</h1>
<ul>
<li>Supervised Learning basics: nearest neighbors, decision trees, linear classifiers, and simple Bayesian classifiers; feature processing and selection; avoiding over-fitting; experimental evaluation.</li>
<li>Unsupervised learning: clustering algorithms; generative probabilistic models; the EM algorithm; association rules.</li>
<li>Theory: basic PAC analysis for classification.</li>
<li>More supervised learning: neural networks; backpropagation; dual perceptron; kernel methods; support vector machines.</li>
<li>Additional topics selected from: active learning; aggregation methods (boosting and bagging); time series models (HMM); reinforcement learning</li>
</ul>
<h1 id="reference-material">Reference Material</h1>
<p>We will use a mixture of primary research materials, portions of texts, and online sources. Required reading material will be listed as such. The following is a list of recommended reference material.</p>
<ul>
<li>Machine Learning. Tom M. Mitchell, McGraw-Hill, 1997<br /></li>
<li>Introduction to Machine Learning, Ethem Alpaydin, 2010.<br /></li>
<li>An introduction to support vector machines : and other kernel-based learning methods. N. Cristianini and J. Shawe-Taylor, 2000.<br /></li>
<li>Data Mining: Practical Machine Learning Tools and Techniques. Ian H. Witten, Eibe Frank, 2005.</li>
<li>Machine Learning: The Art and Science of Algorithms that Make Sense of Data. Peter Flach, 2012.<br /></li>
<li>Pattern Classification. R. Duda, P. Hart, and D. Stork, 2001.<br /></li>
<li>Artificial Intelligence: A Modern Approach. Stuart Russell and Peter Norvig, 2010<br /></li>
<li>Principles of Data Mining. D. Hand, H. Mannila, and P. Smyth, 2001.</li>
<li>Reinforcement Learning: an Introduction. R. Sutton and A. Barto, 1998.x</li>
</ul>
<h1 id="programming-and-software">Programming and Software</h1>
<p><a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> is a great machine learning package that has been around for a while. It is quite extensible, and we will be using it for some assignments. You can use weka.jar on the CS department servers through the command line. If you have trouble, there is excellent documentation on the <a href="https://weka.wikispaces.com/">Weka wiki</a>.</p>
<p>There are some languages that are particularly useful in the context of machine learning, either because of their innate capabilities or because of libraries implemented in the language. When code examples are provided in class they will likely be in one of these language:</p>
<ul>
<li>Python</li>
<li>Java</li>
<li>Julia</li>
<li>Matlab</li>
<li>Clojure</li>
<li>R</li>
</ul>
<h1 id="schedule">Schedule</h1>
<table>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">Lecture</th>
<th align="left">Assignments and Notes</th>
<th align="left">Due Date</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">01/21</td>
<td align="left">Introduction to Machine Learning</td>
<td align="left"><a href="http://kephale.github.io/TuftsCOMP135_Spring2016/Lecture01">Slides</a> <a href="#assignment1">Assignment 1</a></td>
<td align="left">01/27</td>
</tr>
<tr class="even">
<td align="left">01/26</td>
<td align="left">Instance learning</td>
</tr>
<tr class="odd">
<td align="left">01/28</td>
<td align="left">Decision trees pt 1</td>
</tr>
<tr class="even">
<td align="left">02/02</td>
<td align="left">Decision trees pt 2</td>
</tr>
<tr class="odd">
<td align="left">02/04</td>
<td align="left">Naive bayes</td>
</tr>
<tr class="even">
<td align="left">02/09</td>
<td align="left">Measuring ML success pt 1</td>
</tr>
<tr class="odd">
<td align="left">02/11</td>
<td align="left">Measuring ML success pt 2</td>
</tr>
<tr class="even">
<td align="left">02/16</td>
<td align="left">Features</td>
</tr>
<tr class="odd">
<td align="left">02/18</td>
<td align="left">No class, Monday Schedule</td>
</tr>
<tr class="even">
<td align="left">02/23</td>
<td align="left">Features</td>
</tr>
<tr class="odd">
<td align="left">02/25</td>
<td align="left">Linear threshold units pt 1</td>
</tr>
<tr class="even">
<td align="left">03/01</td>
<td align="left">Linear threshold units pt 2</td>
</tr>
<tr class="odd">
<td align="left">03/03</td>
<td align="left">Clustering pt 1</td>
</tr>
<tr class="even">
<td align="left">03/08</td>
<td align="left">Clustering pt 2</td>
</tr>
<tr class="odd">
<td align="left">03/10</td>
<td align="left">Unsupervised learning</td>
</tr>
<tr class="even">
<td align="left">03/15</td>
<td align="left">Association rules</td>
</tr>
<tr class="odd">
<td align="left">03/17</td>
<td align="left">Midterm</td>
</tr>
<tr class="even">
<td align="left">03/22</td>
<td align="left">No class, Spring recess</td>
</tr>
<tr class="odd">
<td align="left">03/24</td>
<td align="left">No class, Spring recess</td>
</tr>
<tr class="even">
<td align="left">03/29</td>
<td align="left">Computational learning theory</td>
</tr>
<tr class="odd">
<td align="left">03/31</td>
<td align="left">Kernel-based methods</td>
</tr>
<tr class="even">
<td align="left">04/05</td>
<td align="left">Perceptron</td>
</tr>
<tr class="odd">
<td align="left">04/07</td>
<td align="left">SVM</td>
</tr>
<tr class="even">
<td align="left">04/12</td>
<td align="left">Active learning</td>
</tr>
<tr class="odd">
<td align="left">04/14</td>
<td align="left">MDPs and Reinforcement Learning</td>
</tr>
<tr class="even">
<td align="left">04/19</td>
<td align="left">Reinforcement learning pt 2</td>
</tr>
<tr class="odd">
<td align="left">04/21</td>
<td align="left">Aggregation methods</td>
</tr>
<tr class="even">
<td align="left">04/26</td>
<td align="left">Project presentations</td>
</tr>
<tr class="odd">
<td align="left">04/28</td>
<td align="left">Project presentations</td>
</tr>
</tbody>
</table>
<h1 id="assignments">Assignments</h1>
<h2 id="assignment1">Assignment1</h2>
<ul>
<li>Download <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a></li>
<li>Download <a href="http://www.github.com/kephale/TuftsCOMP135_Spring2016/Assignment1/IHME_GBD_2013_LIFE_EXPECTANCY_1970_2013_Y2014M12D17.csv">Dataset</a><br /></li>
<li>Open Weka, Choose Explorer models</li>
<li>Load the dataset with &quot;Open file...&quot;</li>
<li>Investigate the data through the &quot;Classify&quot; &quot;Cluster&quot; &quot;Associate&quot; &quot;Visualize&quot; tabs<br /></li>
<li>If Weka is running slowly, then you can try the <a href="http://www.github.com/kephale/TuftsCOMP135_Spring2016/Assignment1/IHME_GBD_2013_LIFE_EXPECTANCY_1970_2013_Y2014M12D17_short.csv">abbreviated dataset</a></li>
</ul>
<p>Note that it is possible to call Weka from the command line (i.e. on the homework server)</p>
<h3 id="submission-of-assignment-1">Submission of assignment 1</h3>
<p>Write a one paragraph description of what you can find.</p>
<ul>
<li>Open &quot;Visualize&quot; and investigate how pairs of attributes relate to each other?</li>
<li>What types of clusters can you find (try &quot;Cluster&quot;/&quot;Choose&quot;/&quot;SimpleKMeans&quot; test with different &quot;numClusters&quot;)</li>
<li>If you're feeling adventurous, then try to build a classifier (&quot;Classify&quot;/&quot;Choose&quot;/&quot;weka.classifiers.trees.J48&quot; and choose a nominal attribute to classify over, like &quot;location_name&quot;. In the case of &quot;location_name&quot;, before building the classifier use &quot;Preprocess&quot; and remove all &quot;location&quot; attributes except &quot;location_name&quot;. You will want to use the abbreviated dataset for this.)</li>
</ul>
